{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "This project was a part of the Udacity Data Analyst Nanodegree Program. It was meant to assure us have mastered the subjects covered in the statistics lessons.\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "- [Conclusions](#conclusions)\n",
    "- [References](#references)\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists. This project is about analyzing the results of an A/B test run by an e-commerce website. The goal is to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Read in the ab_data.csv data. Store it in df.\n",
    "\n",
    "a. Read in the dataset and take a look at the top few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use the below cell to find the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294478 non-null  int64 \n",
      " 1   timestamp     294478 non-null  object\n",
      " 2   group         294478 non-null  object\n",
      " 3   landing_page  294478 non-null  object\n",
      " 4   converted     294478 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. The proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12104245244060237"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('converted == 1')['user_id'].nunique() / df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. The number of times the `new_page` and `treatment` don't line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` For the rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we cannot be sure if this row truly received the new or old page. We should only use the rows that we can feel confident in the accuracy of the data. Therefore, we will remove the rows that are not aligned.\n",
    "\n",
    "a. Create a new dataset that meets the specifications and store a new dataframe **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Double check all of the correct rows were removed - this should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Explore **df2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. The number of unique **user_id**s in **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b. There is one **user_id** repeated in **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2862    773192\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2['user_id'].duplicated()==True]['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The row information for the repeat **user_id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2862   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2['user_id'].duplicated()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Remove **one** of the rows with a duplicate **user_id**, as we shouldn't be counting the same user more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop_duplicates(subset=['user_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Analyze **df2**.\n",
    "\n",
    "a. The probability of an individual converting regardless of the page they receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df2.query('converted == 1')['user_id'].nunique() / df2['user_id'].nunique()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that the individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_cont = df2.query('group == \"control\"').query('converted == 1')['user_id'].nunique() / df2.query('group == \"control\"')['user_id'].nunique()\n",
    "p_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_treat = df2.query('group == \"treatment\"').query('converted == 1')['user_id'].nunique() / df2.query('group == \"treatment\"')['user_id'].nunique()\n",
    "p_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d .Calculate the observed difference in converted success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_diff = p_treat - p_cont\n",
    "obs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. The probability that an individual received the new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.query('landing_page == \"new_page\"')['user_id'].nunique()/ df2['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If this value differs far from 0.5, we need to do the sanity check for the population sizing invariants with a specific confidence level (e.g. 95%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check: Population sizing invariants\n",
    "Check whether the treatment population and the control population are actually comparable. According to the Central Limit Theorem, we can assume the distribution of the data is normal because the sample size is large enough (290584). If the experiment is set up correctly, the true probability should be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the value of critical z\n",
    "def get_z_crit(alpha):\n",
    "    return -norm.ppf(alpha / 2) # two-tailed test\n",
    "\n",
    "# Compute the confidence interval around the true probability of 0.5\n",
    "# Calculate the observed proprotion of unique user in the treatment group\n",
    "def sanity_check(N_cont, N_exp, alpha, p=0.5):\n",
    "    SE = np.sqrt(p * (1 - p) / (N_cont + N_exp))\n",
    "    MOE = SE * get_z_crit(alpha)\n",
    "    lb, ub = p - MOE, p + MOE\n",
    "    # Observed proportion\n",
    "    phat = N_exp / (N_cont + N_exp)\n",
    "    if (phat >= lb) & (phat <= ub):\n",
    "        return print('Confidence Interval:\\n{}, {}'.format(lb, ub)), print('Observed proportion: {}'.format(phat)), print('Sanity check passed')\n",
    "    else:\n",
    "        return print('Confidence Interval:\\n{}, {}'.format(lb, ub)), print('Observed proportion: {}'.format(phat)), print('Sanity check failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval:\n",
      "0.4981820486936518, 0.5018179513063482\n",
      "Observed proportion: 0.5000619442226688\n",
      "Sanity check passed\n"
     ]
    }
   ],
   "source": [
    "Ncont = df2.query('group == \"control\"')['user_id'].nunique()\n",
    "Nexp = df2.query('group == \"treatment\"')['user_id'].nunique()\n",
    "sanity_check(Ncont, Nexp, 0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sanity Check passed: We can be confident that 95% of the time, the observed proportion of user falls within the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Considering the results above, is there sufficient evidence to say that the new treatment page leads to more conversions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is not sufficient evidence to say that the new treatment page leads to more conversion than old page. In this dataset, the converted rate in the control (old page) group is higher than the converted rate in the treatment (new page) group by about 0.16%. This small difference could appear by chance. In order to figure out whether this difference is significant and not just due to chance, we need to do a hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Notice that because of the time stamp associated with each event, we could technically run a hypothesis test continuously as each observation was observed.  However, then the hard question is do we stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do we run to render a decision that neither page is better than another?  These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "`1.` For now, we need to make the decision just based on all the data provided. If we want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should the null and alternative hypotheses be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$$H_0: p_{new}-p_{old} \\le 0$$**\n",
    "\n",
    "**$$H_1: p_{new}-p_{old} \\gt 0$$**\n",
    "\n",
    "**$p_{old}$** and **$p_{new}$** are the converted success rates for the old and new pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page.\n",
    "- Use a sample size for each page equal to the ones in **ab_data.csv**.\n",
    "- Perform the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df2['converted'].mean() # The converted rate in ab_data.csv regardless of the page.\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. The **converted rate** for $p_{new}$ under the null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_new = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The **converted rate** for $p_{old}$ under the null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_old = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Size of new_page group $n_{new}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_new = df2.query('landing_page == \"new_page\"')['user_id'].nunique()\n",
    "n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Size of old_page group $n_{old}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_old = df2.query('landing_page == \"old_page\"')['user_id'].nunique()\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulate $n_{new}$ transactions with a convert rate of $p_{new}$ under the null.  Store these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_page_converted = np.random.choice([0, 1], size = n_new, replace = True, p = [1-p_new, p_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulate $n_{old}$ transactions with a convert rate of $p_{old}$ under the null.  Store these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_page_converted = np.random.choice([0, 1], size = n_old, replace = True, p = [1-p_old, p_old])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Difference in converted rate between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001853292545473506"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page_converted.mean() - old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This value is very near to zero. Here we are looking at a null where there is no difference in conversion based on the page, which means the conversions for each page are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Simulate 10,000 $p_{new}$ - $p_{old}$ values and store them in a numpy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diffs = []\n",
    "\n",
    "# Simulate sampling distribution of the difference in converted rate under the null hypothesis\n",
    "new_page_converted_simulation = np.random.binomial(n_new, p_new, size=10000)/n_new\n",
    "old_page_converted_simulation = np.random.binomial(n_old, p_old, size=10000)/n_old\n",
    "p_diffs = new_page_converted_simulation - old_page_converted_simulation\n",
    "    \n",
    "# Convert to numpy array\n",
    "p_diffs = np.array(p_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plot sampling distribution of the difference in converted rate **p_diffs** under the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_diffs)\n",
    "plt.axvline(x=obs_diff, color = 'red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6403905215631763e-06"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is what we expected as the mean of the sampling distribution of the difference in conversion rate is very close to 0, if the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. Proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9082"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute p-value\n",
    "(p_diffs>obs_diff).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. What does p-value mean in terms of whether or not there is a difference between the new and old pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We compare the observed difference to the values from the null hypothesis and compute the proportion of the values in the null distribution that are considered extreme based on the alternative hypothesis. This value is called p-value. In other words, it is the probability of the observed difference in converted rate occuring or a difference even more in favor of the converted rate of the new page is better than the old page, given that the converted rate of new page is actually lower than or equal to the converted rate of old page.\n",
    "\n",
    "> As the p value is bigger than 0.05, with a Type I error rate of 5%, we do not have enough evidence that the new page is better than the new page. The observed difference in the converted rate between the two groups is not statistically significant. We fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. We could also use a built-in to achieve similar results.  Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = df2.query('landing_page == \"old_page\"')['converted'].sum()\n",
    "convert_new = df2.query('landing_page == \"new_page\"')['converted'].sum()\n",
    "n_old = df2.query('landing_page == \"old_page\"').shape[0]\n",
    "n_new = df2.query('landing_page == \"new_page\"').shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. Use [`stats.proportions_ztest`](http://knowledgetack.com/python/statsmodels/proportions_ztest/) to compute your test statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value = 0.905\n",
      "z score = -1.311\n"
     ]
    }
   ],
   "source": [
    "convert = np.array([convert_new, convert_old])\n",
    "nobs = np.array([n_new, n_old])\n",
    "stat, pval = sm.stats.proportions_ztest(convert, nobs, alternative = 'larger')\n",
    "print('p_value = {0:0.3f}'.format(pval))\n",
    "print('z score = {0:0.3f}'.format(stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. What do the z-score and p-value computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The z-score gives us an idea of how far from the mean of the normal distribution a data point is. In the other words, itâ€™s a measure of how many standard deviations below or above the mean. In this case, the observed difference in converted rate is 1.311 standard deviation below the mean of the sampling distribution under the null, which is 0. With a sampling distribution, a z-score of less than the z-critical value of ~1.6 means that we are likely to sample such a sample from the sampling distribution given an alpha of 0.05.\n",
    "\n",
    "> We are getting similar p-values where both p-values are above the significance level of 0.05. Again, we failed to reject the null hypothesis with a Type I error rate of 5%. We have evidence that the observed difference in converted rate was likely to come from the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` The result acheived in the previous A/B test can also be acheived by performing regression.<br><br>\n",
    "\n",
    "a. Since each row is either a conversion or no conversion, we should perform logistic regression in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The goal is to use **statsmodels** to fit the logistic regression model to see if there is a significant difference in conversion based on which page a customer receives.  However, we first need to create a column for the intercept, and create a dummy variable column for which page each user received.  Add an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['intercept'] = 1\n",
    "df2['ab_page'] = df2['group'].apply(lambda x: 1 if x == 'treatment' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use **statsmodels** to import the regression model. Instantiate the model, and fit the model using the two columns created in part **b.** to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# Fit linear model and obtain the results\n",
    "lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\n",
    "result = lm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 22 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:24:12</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Thu, 22 Oct 2020   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        11:24:12   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.015113064615719"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.exp(-0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Predicted conversion in old page is 1.015 times more likely than in new page but this difference is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The hypotheses in the regression model are:\n",
    "\n",
    "**$$H_0: p_{new}-p_{old} = 0$$**\n",
    "\n",
    "**$$H_1: p_{new}-p_{old} \\neq 0$$**\n",
    "\n",
    "> This is a two-sided test. Null hypothesis implies that there is no relationship between the probability of conversion and ab_page, where the estimate is equal to zero. In other words, there is no difference in conversion rate between the two groups. The alternative is that the difference in conversion rate is not zero.\n",
    "\n",
    "> In Part II, that was a one-sided test. Null hypothesis implies that conversion rate of the new page is less than or equal to the conversion rate of the old page, whereas the alternative is that conversion rate of the new page is better than the conversion rate of the old page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Now, considering other things that might influence whether or not an individual converts.  Why it is a good idea to consider other factors to add into your regression model.  Are there any disadvantages to adding additional terms into your regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In order to predict whether or not an individual converts, we need to have the individual factors as the predictors in the regression model. These predictors could be gender, age, occupation, country etc.\n",
    "\n",
    "> Though adding additional terms like higher order terms might allow us to better predict the response, adding these terms makes interpreting the results of the data more complex. We should only do it when we specifically see the relationship between the predictors, having an interaction effect on the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives.\n",
    "\n",
    "Does it appear that country had an impact on conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.read_csv('./countries.csv')\n",
    "df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dummy variables\n",
    "dummies = pd.get_dummies(df_new['country'])\n",
    "df_new = df_new.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                   timestamp      group landing_page  \\\n",
       "user_id                                                               \n",
       "834778       UK  2017-01-14 23:08:43.304998    control     old_page   \n",
       "928468       US  2017-01-23 14:44:16.387854  treatment     new_page   \n",
       "822059       UK  2017-01-16 14:04:14.719771  treatment     new_page   \n",
       "711597       UK  2017-01-22 03:14:24.763511    control     old_page   \n",
       "710616       UK  2017-01-16 13:14:44.000513  treatment     new_page   \n",
       "\n",
       "         converted  intercept  ab_page  CA  UK  US  \n",
       "user_id                                             \n",
       "834778           0          1        0   0   1   0  \n",
       "928468           0          1        1   0   0   1  \n",
       "822059           1          1        1   0   1   0  \n",
       "711597           0          1        0   0   1   0  \n",
       "710616           0          1        1   0   1   0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 22 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:24:14</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9893</td> <td>    0.009</td> <td> -223.763</td> <td> 0.000</td> <td>   -2.007</td> <td>   -1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0408</td> <td>    0.027</td> <td>   -1.516</td> <td> 0.130</td> <td>   -0.093</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0099</td> <td>    0.013</td> <td>    0.743</td> <td> 0.457</td> <td>   -0.016</td> <td>    0.036</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Thu, 22 Oct 2020   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        11:24:14   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9893      0.009   -223.763      0.000      -2.007      -1.972\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "CA            -0.0408      0.027     -1.516      0.130      -0.093       0.012\n",
       "UK             0.0099      0.013      0.743      0.457      -0.016       0.036\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit linear model and obtain the results\n",
    "lm2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']])\n",
    "result2 = lm2.fit()\n",
    "result2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.136795\n",
       "ab_page      0.985168\n",
       "CA           0.960062\n",
       "UK           1.009932\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(result2.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0415993967056294"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/0.960062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0150553002127556"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/0.985168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Results Intepretation**\n",
    "- Predicted conversion in old page is 1.015 times more likely than in new page.\n",
    "- The individual living in UK is predicted to be 1.01 times more likely to convert compared to the individual living in US, regardless of the type of pages. \n",
    "- The individual living in US is predicted to be 1.04 times more likely to convert compared to the individual living in CA, regardless of the type of pages.\n",
    "\n",
    "> All these differences is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Though we have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if there significant effects on conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['CA_ab_page'] = df_new['CA']*df_new['ab_page']\n",
    "df_new['UK_ab_page'] = df_new['UK']*df_new['ab_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>CA_ab_page</th>\n",
       "      <th>UK_ab_page</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                   timestamp      group landing_page  \\\n",
       "user_id                                                               \n",
       "834778       UK  2017-01-14 23:08:43.304998    control     old_page   \n",
       "928468       US  2017-01-23 14:44:16.387854  treatment     new_page   \n",
       "822059       UK  2017-01-16 14:04:14.719771  treatment     new_page   \n",
       "711597       UK  2017-01-22 03:14:24.763511    control     old_page   \n",
       "710616       UK  2017-01-16 13:14:44.000513  treatment     new_page   \n",
       "\n",
       "         converted  intercept  ab_page  CA  UK  US  CA_ab_page  UK_ab_page  \n",
       "user_id                                                                     \n",
       "834778           0          1        0   0   1   0           0           0  \n",
       "928468           0          1        1   0   0   1           0           0  \n",
       "822059           1          1        1   0   1   0           0           1  \n",
       "711597           0          1        0   0   1   0           0           0  \n",
       "710616           0          1        1   0   1   0           0           1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 22 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:24:15</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>   -1.9865</td> <td>    0.010</td> <td> -206.344</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>    <td>   -0.0206</td> <td>    0.014</td> <td>   -1.505</td> <td> 0.132</td> <td>   -0.047</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>         <td>   -0.0175</td> <td>    0.038</td> <td>   -0.465</td> <td> 0.642</td> <td>   -0.091</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>         <td>   -0.0057</td> <td>    0.019</td> <td>   -0.306</td> <td> 0.760</td> <td>   -0.043</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA_ab_page</th> <td>   -0.0469</td> <td>    0.054</td> <td>   -0.872</td> <td> 0.383</td> <td>   -0.152</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK_ab_page</th> <td>    0.0314</td> <td>    0.027</td> <td>    1.181</td> <td> 0.238</td> <td>   -0.021</td> <td>    0.084</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Thu, 22 Oct 2020   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        11:24:15   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1920\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9865      0.010   -206.344      0.000      -2.005      -1.968\n",
       "ab_page       -0.0206      0.014     -1.505      0.132      -0.047       0.006\n",
       "CA            -0.0175      0.038     -0.465      0.642      -0.091       0.056\n",
       "UK            -0.0057      0.019     -0.306      0.760      -0.043       0.031\n",
       "CA_ab_page    -0.0469      0.054     -0.872      0.383      -0.152       0.059\n",
       "UK_ab_page     0.0314      0.027      1.181      0.238      -0.021       0.084\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit linear model and obtain the results\n",
    "lm3 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK', 'CA_ab_page', 'UK_ab_page']])\n",
    "result3 = lm3.fit()\n",
    "result3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Results interpretation of the interaction between dummy coded categorical variables**\n",
    "- With the interaction the following six groups were generated: CA_new_page, UK_new_page, US_new_page, CA_old_page, UK_old_page, US_old_page\n",
    "    - Coefficient for US_old_page: intercept\n",
    "    - Coefficient for US_new_page: intercept + ab_page\n",
    "    - Coefficient for CA_old_page: intercept + CA\n",
    "    - Coefficient for CA_new_page: intercept + CA + ab_page + CA_ab_page\n",
    "    - Coefficient for UK_old_page: intercept + UK\n",
    "    - Coefficient for UK_new_page: intercept + UK + ab_page + UK_ab_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Page\n",
      "CA: 3.053646594031197\n",
      "UK: 3.142992533885524\n",
      "US: 1.116824153624537\n",
      "\n",
      "Old Page\n",
      "CA: 1.1198026370026495\n",
      "UK: 1.1314502092167007\n",
      "US: 0.13717773852380238\n"
     ]
    }
   ],
   "source": [
    "coef = np.exp(result3.params)\n",
    "\n",
    "CA_new = coef[0] + coef[1] + coef[2] + coef[4]\n",
    "UK_new = coef[0] + coef[1] + coef[3] + coef[5]\n",
    "US_new = coef[0] + coef[1]\n",
    "\n",
    "CA_old = coef[0] + coef[2]\n",
    "UK_old = coef[0] + coef[3]\n",
    "US_old = coef[0]\n",
    "\n",
    "print('New Page')\n",
    "print('CA: {}'.format(CA_new))\n",
    "print('UK: {}'.format(UK_new))\n",
    "print('US: {}'.format(US_new))\n",
    "print('')\n",
    "print('Old Page')\n",
    "print('CA: {}'.format(CA_old))\n",
    "print('UK: {}'.format(UK_old))\n",
    "print('US: {}'.format(US_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the results, adding interaction terms is not useful in predicting the probability of conversion. For the interaction terms, the associated p-values are not statistcally significant. There is no interection between the type of page and the country, e.g. a user living in Canada isn't statistically significant more likely to convert when they see the old page than when they see the new page. Hence, we failed to reject the null hypothesis. This indicates that it is likely to observe such insubstantial association between the predictors and the response due to chance, meaning that whether or not an individuals converts does not depend on the pages they see and which country are they from. However, this is still a valuable insight that hardens the findings of prior tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick glimpse at the conversion rates by date, weekday and hour to see whether there is a specific period of time that the users convert more than the other.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['timestamp'] = pd.to_datetime(df_new['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp   landing_page\n",
       "2017-01-02  new_page        0.119874\n",
       "            old_page        0.125568\n",
       "2017-01-03  new_page        0.113781\n",
       "            old_page        0.113809\n",
       "2017-01-04  new_page        0.116649\n",
       "            old_page        0.121922\n",
       "2017-01-05  new_page        0.114988\n",
       "            old_page        0.123230\n",
       "2017-01-06  new_page        0.123462\n",
       "            old_page        0.115350\n",
       "2017-01-07  new_page        0.116205\n",
       "            old_page        0.120987\n",
       "2017-01-08  new_page        0.120746\n",
       "            old_page        0.118887\n",
       "2017-01-09  new_page        0.118065\n",
       "            old_page        0.119644\n",
       "2017-01-10  new_page        0.126344\n",
       "            old_page        0.112864\n",
       "2017-01-11  new_page        0.115091\n",
       "            old_page        0.118870\n",
       "2017-01-12  new_page        0.122344\n",
       "            old_page        0.122048\n",
       "2017-01-13  new_page        0.111248\n",
       "            old_page        0.116911\n",
       "2017-01-14  new_page        0.119260\n",
       "            old_page        0.126756\n",
       "2017-01-15  new_page        0.113452\n",
       "            old_page        0.120494\n",
       "2017-01-16  new_page        0.119175\n",
       "            old_page        0.121833\n",
       "2017-01-17  new_page        0.127256\n",
       "            old_page        0.122865\n",
       "2017-01-18  new_page        0.124792\n",
       "            old_page        0.124807\n",
       "2017-01-19  new_page        0.117216\n",
       "            old_page        0.119945\n",
       "2017-01-20  new_page        0.117682\n",
       "            old_page        0.115243\n",
       "2017-01-21  new_page        0.115701\n",
       "            old_page        0.125945\n",
       "2017-01-22  new_page        0.118009\n",
       "            old_page        0.119163\n",
       "2017-01-23  new_page        0.121061\n",
       "            old_page        0.125670\n",
       "2017-01-24  new_page        0.121706\n",
       "            old_page        0.118007\n",
       "Name: converted, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.groupby([df_new['timestamp'].dt.date, 'landing_page'])['converted'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp  landing_page\n",
       "0          new_page        0.119491\n",
       "           old_page        0.122795\n",
       "1          new_page        0.122339\n",
       "           old_page        0.116748\n",
       "2          new_page        0.118837\n",
       "           old_page        0.121835\n",
       "3          new_page        0.118209\n",
       "           old_page        0.121729\n",
       "4          new_page        0.117538\n",
       "           old_page        0.115834\n",
       "5          new_page        0.117058\n",
       "           old_page        0.124567\n",
       "6          new_page        0.117431\n",
       "           old_page        0.119518\n",
       "Name: converted, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.groupby([df_new['timestamp'].dt.weekday, 'landing_page'])['converted'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp  landing_page\n",
       "0          new_page        722\n",
       "           old_page        711\n",
       "1          new_page        673\n",
       "           old_page        720\n",
       "2          new_page        681\n",
       "           old_page        687\n",
       "3          new_page        716\n",
       "           old_page        678\n",
       "4          new_page        640\n",
       "           old_page        731\n",
       "5          new_page        725\n",
       "           old_page        755\n",
       "6          new_page        691\n",
       "           old_page        772\n",
       "7          new_page        701\n",
       "           old_page        729\n",
       "8          new_page        719\n",
       "           old_page        718\n",
       "9          new_page        744\n",
       "           old_page        754\n",
       "10         new_page        710\n",
       "           old_page        714\n",
       "11         new_page        721\n",
       "           old_page        772\n",
       "12         new_page        757\n",
       "           old_page        746\n",
       "13         new_page        749\n",
       "           old_page        700\n",
       "14         new_page        715\n",
       "           old_page        728\n",
       "15         new_page        725\n",
       "           old_page        718\n",
       "16         new_page        701\n",
       "           old_page        740\n",
       "17         new_page        745\n",
       "           old_page        746\n",
       "18         new_page        780\n",
       "           old_page        756\n",
       "19         new_page        727\n",
       "           old_page        694\n",
       "20         new_page        726\n",
       "           old_page        702\n",
       "21         new_page        743\n",
       "           old_page        711\n",
       "22         new_page        735\n",
       "           old_page        720\n",
       "23         new_page        718\n",
       "           old_page        787\n",
       "Name: converted, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.groupby([df_new['timestamp'].dt.hour, 'landing_page'])['converted'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that the conversion rates are pretty similar across the date, weekday and hour of the day. There isn't a day or a particular period of the day is standout. We could also fit them into the logistic regression model to see if there is a significant effect on conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>CA_ab_page</th>\n",
       "      <th>UK_ab_page</th>\n",
       "      <th>day</th>\n",
       "      <th>weekdays</th>\n",
       "      <th>weekends</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                  timestamp      group landing_page  converted  \\\n",
       "user_id                                                                         \n",
       "834778       UK 2017-01-14 23:08:43.304998    control     old_page          0   \n",
       "928468       US 2017-01-23 14:44:16.387854  treatment     new_page          0   \n",
       "822059       UK 2017-01-16 14:04:14.719771  treatment     new_page          1   \n",
       "711597       UK 2017-01-22 03:14:24.763511    control     old_page          0   \n",
       "710616       UK 2017-01-16 13:14:44.000513  treatment     new_page          0   \n",
       "\n",
       "         intercept  ab_page  CA  UK  US  CA_ab_page  UK_ab_page  day  \\\n",
       "user_id                                                                \n",
       "834778           1        0   0   1   0           0           0    5   \n",
       "928468           1        1   0   0   1           0           0    0   \n",
       "822059           1        1   0   1   0           0           1    0   \n",
       "711597           1        0   0   1   0           0           0    6   \n",
       "710616           1        1   0   1   0           0           1    0   \n",
       "\n",
       "         weekdays  weekends  \n",
       "user_id                      \n",
       "834778          0         1  \n",
       "928468          1         0  \n",
       "822059          1         0  \n",
       "711597          0         1  \n",
       "710616          1         0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['day'] = df_new['timestamp'].dt.weekday\n",
    "period = df_new['day'].apply(lambda x: 'weekdays' if x<5 else 'weekends')\n",
    "dummies = pd.get_dummies(period)\n",
    "df_new = df_new.join(dummies)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290581</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 22 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>8.088e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:24:16</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.4230</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9889</td> <td>    0.009</td> <td> -226.066</td> <td> 0.000</td> <td>   -2.006</td> <td>   -1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekends</th>  <td>    0.0006</td> <td>    0.013</td> <td>    0.049</td> <td> 0.961</td> <td>   -0.024</td> <td>    0.026</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290581\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Thu, 22 Oct 2020   Pseudo R-squ.:               8.088e-06\n",
       "Time:                        11:24:16   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.4230\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9889      0.009   -226.066      0.000      -2.006      -1.972\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "weekends       0.0006      0.013      0.049      0.961      -0.024       0.026\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fit linear model and obtain the results\n",
    "lm4 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'weekends']])\n",
    "result4 = lm4.fit()\n",
    "result4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On weekends, conversion isn't significantly different than on weekdays, regardless of the type of page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['hour'] = df_new.timestamp.dt.hour\n",
    "df_new['minute'] = df_new.timestamp.dt.minute \n",
    "\n",
    "# split timestamp into hour and minutes\n",
    "df_new['time_in_min'] = df_new['hour']*60 + df_new['minute']\n",
    "\n",
    "# There are 24 hrs in a day, so we can split these hours into 4 sections\n",
    "bin_edges = [0, 6*60, 12*60, 18*60, 24*60]\n",
    "bin_labels = ['overnight', 'morning', 'afternoon', 'evening']\n",
    "df_new['time_of_day'] = pd.cut(df_new['time_in_min'], bin_edges, labels=bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>...</th>\n",
       "      <th>weekdays</th>\n",
       "      <th>weekends</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>time_in_min</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>overnight</th>\n",
       "      <th>morning</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>evening</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>1388</td>\n",
       "      <td>evening</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>884</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>844</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>194</td>\n",
       "      <td>overnight</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>794</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                  timestamp      group landing_page  converted  \\\n",
       "user_id                                                                         \n",
       "834778       UK 2017-01-14 23:08:43.304998    control     old_page          0   \n",
       "928468       US 2017-01-23 14:44:16.387854  treatment     new_page          0   \n",
       "822059       UK 2017-01-16 14:04:14.719771  treatment     new_page          1   \n",
       "711597       UK 2017-01-22 03:14:24.763511    control     old_page          0   \n",
       "710616       UK 2017-01-16 13:14:44.000513  treatment     new_page          0   \n",
       "\n",
       "         intercept  ab_page  CA  UK  US  ...  weekdays  weekends  hour  \\\n",
       "user_id                                  ...                             \n",
       "834778           1        0   0   1   0  ...         0         1    23   \n",
       "928468           1        1   0   0   1  ...         1         0    14   \n",
       "822059           1        1   0   1   0  ...         1         0    14   \n",
       "711597           1        0   0   1   0  ...         0         1     3   \n",
       "710616           1        1   0   1   0  ...         1         0    13   \n",
       "\n",
       "         minute  time_in_min  time_of_day  overnight  morning afternoon  \\\n",
       "user_id                                                                   \n",
       "834778        8         1388      evening          0        0         0   \n",
       "928468       44          884    afternoon          0        0         1   \n",
       "822059        4          844    afternoon          0        0         1   \n",
       "711597       14          194    overnight          1        0         0   \n",
       "710616       14          794    afternoon          0        0         1   \n",
       "\n",
       "         evening  \n",
       "user_id           \n",
       "834778         1  \n",
       "928468         0  \n",
       "822059         0  \n",
       "711597         0  \n",
       "710616         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df_new['time_of_day'])\n",
    "df_new = df_new.join(dummies)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366100\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290579</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     4</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 22 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>5.992e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:24:17</td>     <th>  Log-Likelihood:    </th> <td>-1.0638e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.01257</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -2.0217</td> <td>    0.013</td> <td> -156.811</td> <td> 0.000</td> <td>   -2.047</td> <td>   -1.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>morning</th>   <td>    0.0419</td> <td>    0.016</td> <td>    2.581</td> <td> 0.010</td> <td>    0.010</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>afternoon</th> <td>    0.0417</td> <td>    0.016</td> <td>    2.571</td> <td> 0.010</td> <td>    0.010</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>evening</th>   <td>    0.0476</td> <td>    0.016</td> <td>    2.930</td> <td> 0.003</td> <td>    0.016</td> <td>    0.079</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290579\n",
       "Method:                           MLE   Df Model:                            4\n",
       "Date:                Thu, 22 Oct 2020   Pseudo R-squ.:               5.992e-05\n",
       "Time:                        11:24:17   Log-Likelihood:            -1.0638e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.01257\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -2.0217      0.013   -156.811      0.000      -2.047      -1.996\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "morning        0.0419      0.016      2.581      0.010       0.010       0.074\n",
       "afternoon      0.0417      0.016      2.571      0.010       0.010       0.074\n",
       "evening        0.0476      0.016      2.930      0.003       0.016       0.079\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fit linear model and obtain the results\n",
    "lm5 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'morning', 'afternoon', 'evening']])\n",
    "result5 = lm5.fit()\n",
    "result5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.132429\n",
       "ab_page      0.985169\n",
       "morning      1.042800\n",
       "afternoon    1.042600\n",
       "evening      1.048706\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(result5.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the results, it shows that users are predicted to convert more in the morning, afternoon and evening than overnight and the p-values associated are all statistically significant. However, the predicted difference between the conversion among these three period of time is not statistically significant, as the confidence intervals overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "\n",
    "> In part II, it was an **aggregate** approach using hypothesis testing to understand whether or not the proportion of users that converts using new page is better than the average using old page. The results showed that there is no significantly difference in conversion rate between the two groups.\n",
    "\n",
    "> In part III,it was an **individual** approach using logistic regression to understand whether or not an individual converts depends on the pages they see and which country are they from. The results showed that having country as additional factor and adding interaction terms into the logistic regression model are not useful in predicting the probability of conversion. \n",
    "- The type of page does not correlate to the user conversion, which agrees with the A/B test. \n",
    "- The country where the user lives is not a good predictor for user conversion. \n",
    "- The interaction between type of page and country has no significant effects on conversion.\n",
    "- Adding time factors worsens the model fit, as the R-squared decreases.\n",
    "\n",
    "> The R-squared value is the highest for the second linear model, where we only fit the model with page and country, without the interaction terms. Even though this model isn't great, it fits better than the others. In other words, adding the interaction terms and time factor provides no improvement in the model. However, these are still valuable insights that hardens the findings of prior tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='references'></a>\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intepretation of the interaction between dummy coded variables: https://www.theanalysisfactor.com/interaction-dummy-variables-in-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate .html file in the workspace directory \n",
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
